{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERT 情感分析微调 (Unsloth 框架 + Yelp 数据集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d797fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: peft in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (0.46.1)\n",
      "Requirement already satisfied: accelerate in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (1.9.0)\n",
      "Requirement already satisfied: tqdm in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: psutil in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from triton==3.3.1->torch>=1.13.0->peft) (78.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# 安装依赖（建议在终端运行）\n",
    "!pip install transformers datasets peft bitsandbytes accelerate tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2509829e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载 unsloth + bert 模型\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0265cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 加载 Amazon US Reviews 数据集 - Electronics 类目 (3C)\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"amazon_polarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39bb32c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行数据预处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing with tqdm (num_proc=4):   0%|          | 1395/3600000 [00:00<05:34, 10765.63 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing with tqdm (num_proc=4): 100%|██████████| 3600000/3600000 [03:41<00:00, 16247.30 examples/s]\n",
      "Tokenizing with tqdm (num_proc=4): 100%|██████████| 400000/400000 [00:24<00:00, 16188.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据预处理完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 数据预处理：使用 content -> text，label -> label\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    result = tokenizer(examples[\"content\"], truncation=True, padding=\"max_length\")\n",
    "    result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "print(\"开始进行数据预处理...\")\n",
    "encoded_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing with tqdm\",\n",
    "    num_proc=4  # 使用多进程加速\n",
    ")\n",
    "print(\"数据预处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de149906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.53.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, trl, unsloth, unsloth_zoo\n",
      "/home/workspace/miniconda3/envs/LLMTraing_py311/bin/python\n",
      "Name: transformers\n",
      "Version: 4.53.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, trl, unsloth, unsloth_zoo\n",
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                   /home/workspace/miniconda3\n",
      "LLMTraing_py311      * /home/workspace/miniconda3/envs/LLMTraing_py311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers\n",
    "!which python\n",
    "!pip show transformers\n",
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699ed487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/workspace/miniconda3/envs/LLMTraing_py311/lib/python3.11/site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730388ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset.set_format(\"torch\")\n",
    "from transformers import __version__\n",
    "print(__version__)\n",
    "\n",
    "# 4. 减少训练数据规模用于快速调试\n",
    "small_train_dataset = encoded_dataset[\"train\"].shuffle(seed=42).select(range(5000))\n",
    "small_eval_dataset = encoded_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# 5. 定义训练参数（调小 batch size，增加训练稳定性）\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_sentiment_amazon_polarity\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,  # 更小 batch size 防止显存溢出\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5006739a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 02:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.457400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.300500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.442600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.191700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.090900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.164200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.081200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.194800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=0.18060689756100376, metrics={'train_runtime': 160.3405, 'train_samples_per_second': 93.551, 'train_steps_per_second': 11.694, 'total_flos': 3946665830400000.0, 'train_loss': 0.18060689756100376, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a8f8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练完成，3C类目模型已保存！\n"
     ]
    }
   ],
   "source": [
    "# 6. 保存模型\n",
    "model.save_pretrained(\"./bert_sentiment_model_3c\")\n",
    "tokenizer.save_pretrained(\"./bert_sentiment_model_3c\")\n",
    "\n",
    "print(\"训练完成，3C类目模型已保存！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d5380f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载测试管道...\n",
      "输入文本: This product is amazing and works perfectly! Highly recommend.\n",
      "预测情感: Positive (置信度: 1.00)\n",
      "输入文本: Terrible experience. Completely useless product.\n",
      "预测情感: Negative (置信度: 1.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 'Negative', 'score': 0.9995960593223572}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. 测试模型\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "print(\"加载测试管道...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./bert_sentiment_model_3c\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert_sentiment_model_3c\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "def test_sentiment(text):\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    label_id = int(result['label'].split('_')[-1])\n",
    "    label = id2label[label_id]\n",
    "    print(f\"输入文本: {text}\")\n",
    "    print(f\"预测情感: {label} (置信度: {result['score']:.2f})\")\n",
    "    return {\"label\": label, \"score\": result['score']}\n",
    "\n",
    "# 示例测试\n",
    "example_text = \"This product is amazing and works perfectly! Highly recommend.\"\n",
    "test_sentiment(example_text)\n",
    "\n",
    "example_text2 = \"Terrible experience. Completely useless product.\"\n",
    "test_sentiment(example_text2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMTraing_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
